{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5164a87b-a234-46f2-9e2c-bbd2cdc19412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---- \n",
    "\n",
    "In this tutorial, we will code **Self-Attention** in **[PyTorch](https://pytorch.org/)**. **Attention** is an essential component of neural network **Transformers**, which are driving the current excitement in **Large Language Models** and **AI**. Specifically, an **Enecoder-Only Transformer**, illustrated below, is the foundation for the popular model **BERT**. \n",
    "\n",
    "<img src=\"./images-nb/encoder_only_1.png\" alt=\"an enecoder-only transformer neural network\" style=\"width: 800px;\">\n",
    "\n",
    "At the heart of **BERT** is **Self-Attention**, which allows it to establish relationships among the words, characters and symbols, that are used for input and collectively called **Tokens**. For example, in the illustration below, where the word **it** could potentially refer to either **pizza** or **oven**, **Attention** could help a **Transformer** establish the correctly relationship between the word **it** and **pizza**.\n",
    "\n",
    "<img src=\"./images-nb/attention_ex_1.png\" alt=\"an illustration of how attention works\" style=\"width: 800px;\"/>\n",
    "\n",
    "In this tutorial, you will...\n",
    "\n",
    "- **[Code a Basic Self-Attention Class!!!](#selfAttention)** The basic self-attention class allows the transformer to establish relationships among words and tokens.\n",
    "\n",
    "- **[Calculate Self-Attention Values!!!](#calculate)** We'll then use the class that we created, SelfAttention, to calculate self-attention values for some sample data.\n",
    " \n",
    "- **[Verify The Calculations!!!](#validate)** Lastly, we'll validate the calculations made by the SelfAttention class.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37de0f29-6f41-4e0b-bd25-a457e1552d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F ## This gives us the softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3e7757-74ef-47c7-a07d-b176c6ed0ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Code Self-Attention\n",
    "<a id=\"selfAttention\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8171264d-340f-4248-934a-4e3245b8c1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all neural network modules in PyTorch\n",
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, d_model=2,\n",
    "               row_dim=0,\n",
    "               col_dim=1):\n",
    "    ## d_model = the number of embedding values per token.\n",
    "    ##           Because we want to be able to do the math by hand, we've\n",
    "    ##           the default value for d_model=2.\n",
    "    ##           However, in \"Attention Is All You Need\" d_model=512\n",
    "    ##\n",
    "    ## row_dim, col_dim = the indices we should use to access rows or columns\n",
    "    super().__init__()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "334eefe5-6a0b-414f-9587-26b1042659f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Calculate Self-Attention\n",
    "<a id=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd8a8635-5ef6-41ba-9437-e07faa263bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fa4dbcf-7bc9-4baf-9e67-4f3ee5fc360a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Print Out Weights and Verify Calculations\n",
    "<a id=\"validate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d19e2af-f147-43fc-90d1-03b6044d1d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "00-self-attention",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
